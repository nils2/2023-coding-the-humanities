{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Web Scraping and APIs\n",
    "\n",
    "In this notebook, we learn how to scrape data from the Web and get an idea of what Applicaiton Programming Interfaces are (APIs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "**Web Scraping** is a technique for the extraction of information from websites by transforming unstructured data (HTML pages) into structured data (databases or spreadsheets). \n",
    "\n",
    "Even if scraping can be manually performed by a user, it is usually implemented using a **web crawler** (i.e., it is usually implemented as an automatic process). For larger scale scraping see, e.g., [Scrapy](https://scrapy.org)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is an alternative to using already available **API**s (Application Programming Interface), such as those provided by all the major platforms, like *Facebook*, *Google* and *Twitter*. **More below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of HTML\n",
    "\n",
    "The **HyperText Markup Language (HTML)** is the standard **descriptive markup** language for web pages.\n",
    "\n",
    "\n",
    "- **Markup** language: a human-readable, explicit system for annotating the content of a document. Markdown is another markup language.\n",
    "\n",
    "\n",
    "- **Descriptive** markup languages (e.g. HTML, XML) are used to annotate the structure or the contents of a document, as opposed to **procedural** markup languages (e.g. TEX, Postscript), whose main goal is to describe how a document should be processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML provides a means to annotate the <strong>structural</strong> elements of documents like (different kinds of) headings, paragraphs, lists, links, images, quotes, tables and so forth. Similarly, even if with fewer options, does Markdown (which we are <em>using</em> *here*, check the code!).\n",
    "\n",
    "HTML tags **do not mark the logical structure** of a document, but only its format (e.g. *this is a table*, *this is a h3-type heading*...). It is up to the browser to then use HTML (plus other information, such as *Cascading Style Sheets*), to render a webpage appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML markup relies on a **fixed inventory of tags**, written by using angle brackets. Some tags, e.g. `<p>...</p>`, surround the marked text, and may include subelements. Other tags, e.g. `<br>` or `<img>` introduce content directly.\n",
    "\n",
    "The following is an example of a web page:\n",
    "\n",
    "```html\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "  <head>\n",
    "    <title>The Adventures of Pinocchio</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h2>Carlo Collodi</h2>\n",
    "    <h1>The Adventures of Pinocchio</h1>\n",
    "    <hr>\n",
    "    <h4>CHAPTER 1</h4>\n",
    "    <br>\n",
    "    <p><i>How it happened that Mastro Cherry, carpenter, found a piece of wood that wept and laughed like a child</i></p>\n",
    "    <br>\n",
    "    <p>Centuries ago there lived--</p>\n",
    "    <p>\"A king!\" my little readers will say immediately.</p>\n",
    "  </body>\n",
    "</html>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Web Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The following notes are roughly based on the **Chapters 1-3** of: Mitchell, R. (2015). [Web Scraping with Python](http://shop.oreilly.com/product/0636920034391.do), O'Reilly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modules and Packages Required for Web Scraping\n",
    "\n",
    "**BeautifulSoup**: this library defines [classes and functions](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) to pull data (e.g. table, lists, paragraphs) out of HTML and XML files. It provides idiomatic ways of navigating, searching, and modifying the parse tree.\n",
    "\n",
    "\n",
    "**lxml**: to function, BeautifulSoup relies on external HTML-XML parsers. Many options are available, among which the html5lib's and the Python's built-in parsers. We'll rely on the [lxml](http://lxml.de/)'s parser, due to its high performance, reliability and flexibility.\n",
    "\n",
    "\n",
    "**Urllib**: BeautifulSoup does not fetch the web page for us. To do this, we'll rely on the [Urllib](https://docs.python.org/3.7/library/urllib.html#module-urllib) module available in the Python Standard Library, that implements classes and functions which help in opening URLs (authentication, redirections, cookies and so on). We will see another option, **requests**, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve and Parse an HTML page\n",
    "\n",
    "`urllib.request.urlopen()` allows us to retrieve our target HTML page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if the page doesn't exist?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, let's handle this properly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    html = urlopen(\"http://www.pythonscraping.com/pages/page.html\")\n",
    "except urllib.request.URLError as e:\n",
    "    pass # code your plan B here\n",
    "except urllib.request.URLError as e:\n",
    "    raise # raise any other exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use `BeautifulSoup()` in conjunction with `lxml` to parse out `html` page and store it in the Beautiful Soup format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you might need to to the following:\n",
    "#!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m html \u001b[38;5;241m=\u001b[39m urlopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.pythonscraping.com/pages/page1.html\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m soup_page1 \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhtml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py:245\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "html = urlopen(\"http://www.pythonscraping.com/pages/page1.html\")\n",
    "soup_page1 = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Let's scrape another couple of pages we'll need in our examples\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m soup_page3 \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttp://www.pythonscraping.com/pages/page3.html\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m soup_wap \u001b[38;5;241m=\u001b[39m BeautifulSoup(urlopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://www.pythonscraping.com/pages/warandpeace.html\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py:245\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "#Let's scrape another couple of pages we'll need in our examples\n",
    "soup_page3 = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/page3.html\"), \"lxml\")\n",
    "soup_wap = BeautifulSoup(urlopen(\"http://www.pythonscraping.com/pages/warandpeace.html\"), \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Let's look at the nested structure of the page\n",
    "\n",
    "The `prettify()` method allows us to have a look at the structure of the HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoup_page1\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "print(soup_page1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoup_page1\u001b[49m\u001b[38;5;241m.\u001b[39mprettify())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Let's play with a HTML tag\n",
    "\n",
    "The notation `soup.<tag>` allows us to retrieve the content marked by a tag (opening and closing tags included)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# note that the first \"<div>\" tag is nested two layers deep (html → body → div).\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msoup_page1\u001b[49m\u001b[38;5;241m.\u001b[39mdiv\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "# note that the first \"<div>\" tag is nested two layers deep (html → body → div).\n",
    "soup_page1.div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text is the only thing you're interested into, well, the `soup.<tag>.string` method comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_page1\u001b[49m\u001b[38;5;241m.\u001b[39mdiv\u001b[38;5;241m.\u001b[39mstring\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "soup_page1.div.string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML markup generated by Beautiful Soup can be modified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# let's change the content of our div\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msoup_page1\u001b[49m\u001b[38;5;241m.\u001b[39mdiv\u001b[38;5;241m.\u001b[39mstring \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis content has been changed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# let's change the name of the tag\u001b[39;00m\n\u001b[1;32m      4\u001b[0m soup_page1\u001b[38;5;241m.\u001b[39mdiv\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_div\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "# let's change the content of our div\n",
    "soup_page1.div.string = \"this content has been changed\"\n",
    "# let's change the name of the tag\n",
    "soup_page1.div.name = \"new_div\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoup_page1\u001b[49m\u001b[38;5;241m.\u001b[39mprettify())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "print(soup_page1.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In its simplest use, the `find()` method is an alternative to the `soup.<tag>` notation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_page1\u001b[49m\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_div\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "soup_page1.find(\"new_div\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_page1\u001b[49m\u001b[38;5;241m.\u001b[39mnew_div\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page1' is not defined"
     ]
    }
   ],
   "source": [
    "soup_page1.new_div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but this function allows for the searching of nodes by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_wap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoup_wap\u001b[49m\u001b[38;5;241m.\u001b[39mprettify())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_wap' is not defined"
     ]
    }
   ],
   "source": [
    "print(soup_wap.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_wap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_wap\u001b[49m\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m, attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_wap' is not defined"
     ]
    }
   ],
   "source": [
    "soup_wap.find(\"span\", attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of an attribute for a given tag instance can be retrieved by using the `get(\"ATTRIBUTE\")` method. For instance, if we want to retrieve the URL of an image we can extract the `src` value from the corresponding `<img>` tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_page3\u001b[49m\u001b[38;5;241m.\u001b[39mimg\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page3' is not defined"
     ]
    }
   ],
   "source": [
    "soup_page3.img.get(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to know all the attibutes associated with a given tag, the `attrs` method is convenient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_page3\u001b[49m\u001b[38;5;241m.\u001b[39mimg\u001b[38;5;241m.\u001b[39mattrs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page3' is not defined"
     ]
    }
   ],
   "source": [
    "soup_page3.img.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# by returning a dictionary, it is easy to see how \"attrs\" can be used as an alternative to \"get()\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msoup_page3\u001b[49m\u001b[38;5;241m.\u001b[39mimg\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page3' is not defined"
     ]
    }
   ],
   "source": [
    "# by returning a dictionary, it is easy to see how \"attrs\" can be used as an alternative to \"get()\"\n",
    "soup_page3.img.attrs[\"src\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_page3' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# if you fancy another way to do the same thing...\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msoup_page3\u001b[49m\u001b[38;5;241m.\u001b[39mimg[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_page3' is not defined"
     ]
    }
   ],
   "source": [
    "# if you fancy another way to do the same thing...\n",
    "soup_page3.img[\"src\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with multiple HTML tags at once\n",
    "\n",
    "When the same tag is used multiple time in the same page, however, both the `soup.<tag>` notation and the `find()` method allow you to access **only one instance** (i.e. the first):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_wap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msoup_wap\u001b[49m\u001b[38;5;241m.\u001b[39mprettify())[\u001b[38;5;241m180\u001b[39m:\u001b[38;5;241m1190\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_wap' is not defined"
     ]
    }
   ],
   "source": [
    "print(soup_wap.prettify())[180:1190]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_wap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_wap\u001b[49m\u001b[38;5;241m.\u001b[39mspan\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_wap' is not defined"
     ]
    }
   ],
   "source": [
    "soup_wap.span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the **sequence of all the instances of a tag** in a file, we can use the `find_all()` method (previously known as `findAll()` and `findChildren()` in BS 3 and BS 2, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_wap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_wap\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_wap' is not defined"
     ]
    }
   ],
   "source": [
    "soup_wap.find_all(\"span\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `find_all()` method as well allows for  the extraction of  all tags by exploiting cues in the markup, such as a given **class attribute** value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup_wap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msoup_wap\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m\"\u001b[39m,  attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup_wap' is not defined"
     ]
    }
   ],
   "source": [
    "soup_wap.find_all(\"span\",  attrs = {\"class\":\"green\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Crawling\n",
    "\n",
    "Web Crawlers are softwares designed to collect pages from the Web. In essence, they recursively implement the following steps: \n",
    "\n",
    "- they start by retrieving the page content for an URL \n",
    "\n",
    "\n",
    "- they then parse it to retrieve other URLs of interest\n",
    "\n",
    "\n",
    "- they then focus on these new URLs, for each of which they repeat the whole process, ad infinitum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you want to crawl and **entire site**:\n",
    "\n",
    "- start with a top-level page\n",
    "\n",
    "\n",
    "- parse the page (retrieve the data your application need) and extract all the internal links, by ignoring already visited URLs\n",
    "\n",
    "\n",
    "- for each new link, move to the corresponding page and repeat the previous step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Random walk through Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set our starting page URL, fetch it and parse its HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m starting_page \u001b[38;5;241m=\u001b[39m urlopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/Chris_Cornell\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstarting_page\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py:245\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "starting_page = urlopen(\"https://en.wikipedia.org/wiki/Chris_Cornell\")\n",
    "soup = BeautifulSoup(starting_page, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it should be easy to extract all the links in the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# links are defined by <a> tag\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link_element \u001b[38;5;129;01min\u001b[39;00m \u001b[43msoup\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(link_element)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "# links are defined by <a> tag\n",
    "for link_element in soup.find_all(\"a\")[:10]:\n",
    "    print(link_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore all the \"a\" tags without an \"href\" attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link_element \u001b[38;5;129;01min\u001b[39;00m [tag \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[43msoup\u001b[49m\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tag\u001b[38;5;241m.\u001b[39mattrs][:\u001b[38;5;241m10\u001b[39m]:\n\u001b[1;32m      3\u001b[0m     url \u001b[38;5;241m=\u001b[39m link_element\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "for link_element in [tag for tag in soup.find_all(\"a\") if 'href' in tag.attrs][:10]:\n",
    "    \n",
    "    url = link_element.attrs['href']\n",
    "    \n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia is full of sidebar, footer, and header links that appear on every page, along with links to the category pages, talk pages, and other pages that do not contain different articles:\n",
    "\n",
    "```\n",
    "/wiki/Template_talk:Chris_Cornell\n",
    "```\n",
    "\n",
    "```\n",
    "#cite_note-147\n",
    "```\n",
    "\n",
    "Moreover, we don't want to visit pages outside of Wikipedia:\n",
    "\n",
    "```\n",
    "http://www.chriscornell.com/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relevant links have three thing in common:\n",
    "\n",
    "- they reside within the `div` with the `id` set to `bodyContent`\n",
    "\n",
    "\n",
    "- the URLs do not contain semicolons\n",
    "\n",
    "\n",
    "- the URLs begin with `/wiki/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [29]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m re_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^(/wiki/)((?!:).)*$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m body \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodyContent\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m link \u001b[38;5;129;01min\u001b[39;00m body\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m: re_pattern}):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(link\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re_pattern = re.compile(r\"^(/wiki/)((?!:).)*$\")\n",
    "\n",
    "body = soup.find(\"div\", {\"id\": \"bodyContent\"})\n",
    "\n",
    "for link in body.find_all(\"a\", {'href': re_pattern}):\n",
    "\n",
    "    print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code returns the list of all the Wikipedia articles linked to our starting page. \n",
    "\n",
    "This is not enough, we want to be recursively repeat this process for all these links. That is, we need a function that takes as input a Wikipedia article URL of the form `/wiki/<Article_Name>` and returns a list of all linked articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(article_url):\n",
    "    \"\"\"\n",
    "    Retrieve all URLs on an English Wikipedia article page (e.g. /wiki/Amsterdam).\n",
    "    \n",
    "    This function needs a relative URL on the \n",
    "    http://en.wikipedia.org domain, such as '/wiki/Amsterdam'. \n",
    "    \n",
    "    Args:\n",
    "        article_url (str): URL of a website\n",
    "        \n",
    "    Returns:\n",
    "        bs4.element.ResultSet: bs link elements resultset\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    page = urlopen(\"http://en.wikipedia.org\" + article_url)\n",
    "    soup = BeautifulSoup(page, \"lxml\")\n",
    "    \n",
    "    body = soup.find(\"div\", {\"id\":\"bodyContent\"})\n",
    "    \n",
    "    re_pattern = re.compile(r\"^(/wiki/)((?!:).)*$\")\n",
    "    \n",
    "    links = body.find_all(\"a\", href=re_pattern)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function by calling it in a script that randomly select, for each iteration, a random link and that stops after 10 URLs have been retrieved (or when it bumps into a page without link):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [31]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m links \u001b[38;5;241m=\u001b[39m \u001b[43mget_links\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/wiki/Chris_Cornell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# for testing purposes, we want to do this 10 times\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(links) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36mget_links\u001b[0;34m(article_url)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mRetrieve all URLs on an English Wikipedia article page (e.g. /wiki/Amsterdam).\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m page \u001b[38;5;241m=\u001b[39m urlopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://en.wikipedia.org\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m article_url)\n\u001b[0;32m---> 17\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlxml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m body \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbodyContent\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m     21\u001b[0m re_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^(/wiki/)((?!:).)*$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bs4/__init__.py:245\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m     builder_class \u001b[38;5;241m=\u001b[39m builder_registry\u001b[38;5;241m.\u001b[39mlookup(\u001b[38;5;241m*\u001b[39mfeatures)\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find a tree builder with the features you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m. Do you need to install a parser library?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(features))\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m builder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "links = get_links(\"/wiki/Chris_Cornell\")\n",
    "\n",
    "for _ in range(10):  # for testing purposes, we want to do this 10 times\n",
    "    if len(links) > 0:\n",
    "        new_article = links[random.randint(0, len(links)-1)].attrs[\"href\"]\n",
    "        print(new_article)\n",
    "        \n",
    "        links = get_links(new_article)\n",
    "        \n",
    "    else:\n",
    "        print(\"No links in this page!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.\n",
    "\n",
    "Write code to retrieve the **motto in English** (if the University has one) of the Internationally Ranked Universities in the Netherlands by starting from the following Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/List_of_universities_in_the_Netherlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with APIs\n",
    "\n",
    "An **Application Programming Interface** is a set of protocols that defines how software programs communicate among eachother. Without APIs, we have to scrape the Web or get the data directly. With APIs, we often can get structured data: it is a much more convenient way to work.\n",
    "\n",
    "APIs are a great option in that they implement extensively tested routines (**high reliability**). However, you should spend time in learning how they work and, in some cases, they don't allow you to access the piece of information you may need (**low flexibility**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  # External package: https://requests.readthedocs.io/en/master/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a Google search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tesla\"\n",
    "r = requests.get('https://www.google.com/search', params={'q': query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text/html; charset=utf-8\n",
      "utf-8\n",
      "https://consent.google.com/ml?continue=https://www.google.com/search%3Fq%3DTesla&gl=NL&m=0&pc=srp&uxe=none&hl=nl&src=1\n"
     ]
    }
   ],
   "source": [
    "print(r.headers['content-type'])\n",
    "print(r.encoding)\n",
    "print(r.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html><html lang=\"nl\" dir=\"ltr\"><head><style nonce=\"/i04ejFn0EnmkEBbprNJag\">\\na, a:link, a:visited, a:active, a:hover {\\n  color: #1a73e8;\\n  text-decoration: none;\\n}\\nbody {\\n  font-family: Roboto,RobotoDraft,Helvetica,Arial,sans-serif;\\n  text-align: center;\\n  -ms-text-size-adjust: 100%;\\n  -moz-text-size-adjust: 100%;\\n  -webkit-text-size-adjust: 100%;\\n}\\n.box {\\n  border: 1px solid #dadce0;\\n  box-sizing: border-box;\\n  border-radius: 8px;\\n  margin: 24px auto 5px auto;\\n  max-width: 520px;\\n  padding: 24px;\\n}\\nh1 {\\n  color: #2c2c2c;\\n  font-size: 24px;\\n  hyphens: auto;\\n  margin: 24px 0;\\n}\\np, .sub, .contentText {\\n  color: #5f6368;;\\n  font-size: 14px;\\n  line-height: 20px;\\n  letter-spacing: 0.2px;\\n  text-align: left;\\n}\\n.signin {\\n  text-align: right;\\n}\\n.image {\\n  display: block;\\n  margin: 14px auto;\\n}\\n.basebutton {\\n  border-radius: 4px;\\n  cursor: pointer;\\n  font-family: Roboto,RobotoDraft,Helvetica,Arial,sans-serif;\\n  font-size: 14px;\\n  font-weight: 500;\\n  height: 36px;\\n  margin: 12px 4px 0;\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.\n",
    "\n",
    "1. Inspect the Google search results page and understand how results are displayed.\n",
    "\n",
    "\n",
    "2. Use BeautifulSoup to get the link of the first 10 results of this search out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about using `requests` to query APIs? Easy using the param dictionary. Responses then follow the starndard format of the API (or you can request the one you like if available)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'{\"current_user_url\":\"https://api.github.com/user\",\"current_user_authorizations_html_url\":\"https://github.com/settings/connections/applications{/client_id}\",\"authorizations_url\":\"https://api.github.com/authorizations\",\"code_search_url\":\"https://api.github.com/search/code?q={query}{&page,per_page,sort,order}\",\"commit_search_url\":\"https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}\",\"emails_url\":\"https://api.github.com/user/emails\",\"emojis_url\":\"https://api.github.com/emojis\",\"events_url\":\"https://api.github.com/events\",\"feeds_url\":\"https://api.github.com/feeds\",\"followers_url\":\"https://api.github.com/user/followers\",\"following_url\":\"https://api.github.com/user/following{/target}\",\"gists_url\":\"https://api.github.com/gists{/gist_id}\",\"hub_url\":\"https://api.github.com/hub\",\"issue_search_url\":\"https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}\",\"issues_url\":\"https://api.github.com/issues\",\"keys_url\":\"https://api.github.com/user/keys\",\"label_search_url\":\"https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}\",\"notifications_url\":\"https://api.github.com/notifications\",\"organization_url\":\"https://api.github.com/orgs/{org}\",\"organization_repositories_url\":\"https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}\",\"organization_teams_url\":\"https://api.github.com/orgs/{org}/teams\",\"public_gists_url\":\"https://api.github.com/gists/public\",\"rate_limit_url\":\"https://api.github.com/rate_limit\",\"repository_url\":\"https://api.github.com/repos/{owner}/{repo}\",\"repository_search_url\":\"https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}\",\"current_user_repositories_url\":\"https://api.github.com/user/repos{?type,page,per_page,sort}\",\"starred_url\":\"https://api.github.com/user/starred{/owner}{/repo}\",\"starred_gists_url\":\"https://api.github.com/gists/starred\",\"topic_search_url\":\"https://api.github.com/search/topics?q={query}{&page,per_page}\",\"user_url\":\"https://api.github.com/users/{user}\",\"user_organizations_url\":\"https://api.github.com/user/orgs\",\"user_repositories_url\":\"https://api.github.com/users/{user}/repos{?type,page,per_page,sort}\",\"user_search_url\":\"https://api.github.com/search/users?q={query}{&page,per_page,sort,order}\"}'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = requests.get('https://api.github.com')\n",
    "\n",
    "# raw\n",
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'current_user_url': 'https://api.github.com/user',\n",
       " 'current_user_authorizations_html_url': 'https://github.com/settings/connections/applications{/client_id}',\n",
       " 'authorizations_url': 'https://api.github.com/authorizations',\n",
       " 'code_search_url': 'https://api.github.com/search/code?q={query}{&page,per_page,sort,order}',\n",
       " 'commit_search_url': 'https://api.github.com/search/commits?q={query}{&page,per_page,sort,order}',\n",
       " 'emails_url': 'https://api.github.com/user/emails',\n",
       " 'emojis_url': 'https://api.github.com/emojis',\n",
       " 'events_url': 'https://api.github.com/events',\n",
       " 'feeds_url': 'https://api.github.com/feeds',\n",
       " 'followers_url': 'https://api.github.com/user/followers',\n",
       " 'following_url': 'https://api.github.com/user/following{/target}',\n",
       " 'gists_url': 'https://api.github.com/gists{/gist_id}',\n",
       " 'hub_url': 'https://api.github.com/hub',\n",
       " 'issue_search_url': 'https://api.github.com/search/issues?q={query}{&page,per_page,sort,order}',\n",
       " 'issues_url': 'https://api.github.com/issues',\n",
       " 'keys_url': 'https://api.github.com/user/keys',\n",
       " 'label_search_url': 'https://api.github.com/search/labels?q={query}&repository_id={repository_id}{&page,per_page}',\n",
       " 'notifications_url': 'https://api.github.com/notifications',\n",
       " 'organization_url': 'https://api.github.com/orgs/{org}',\n",
       " 'organization_repositories_url': 'https://api.github.com/orgs/{org}/repos{?type,page,per_page,sort}',\n",
       " 'organization_teams_url': 'https://api.github.com/orgs/{org}/teams',\n",
       " 'public_gists_url': 'https://api.github.com/gists/public',\n",
       " 'rate_limit_url': 'https://api.github.com/rate_limit',\n",
       " 'repository_url': 'https://api.github.com/repos/{owner}/{repo}',\n",
       " 'repository_search_url': 'https://api.github.com/search/repositories?q={query}{&page,per_page,sort,order}',\n",
       " 'current_user_repositories_url': 'https://api.github.com/user/repos{?type,page,per_page,sort}',\n",
       " 'starred_url': 'https://api.github.com/user/starred{/owner}{/repo}',\n",
       " 'starred_gists_url': 'https://api.github.com/gists/starred',\n",
       " 'topic_search_url': 'https://api.github.com/search/topics?q={query}{&page,per_page}',\n",
       " 'user_url': 'https://api.github.com/users/{user}',\n",
       " 'user_organizations_url': 'https://api.github.com/user/orgs',\n",
       " 'user_repositories_url': 'https://api.github.com/users/{user}/repos{?type,page,per_page,sort}',\n",
       " 'user_search_url': 'https://api.github.com/search/users?q={query}{&page,per_page,sort,order}'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# json\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter API (OPT)\n",
    "\n",
    "Two main APIs:\n",
    "\n",
    "* **Streaming API**: a sample of public tweets and events as they published on Twitter, provides only real-time data without limits.\n",
    "\n",
    "\n",
    "* **REST API**: allows to search, follow trends, read author profile and follower data, post / modify. It provides historical data up to a week (for the free account, more by paying), requires a one-time request and has rate limit (varies for different requests and subscriptions).\n",
    "\n",
    "\n",
    "REST APIs (it is a style for developing Web services which is widely used): https://en.wikipedia.org/wiki/Representational_state_transfer\n",
    "\n",
    "Some more basic info: https://developer.twitter.com/en/docs/basics/things-every-developer-should-know\n",
    "\n",
    "Tutorials: https://developer.twitter.com/en/docs/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: authentication\n",
    "\n",
    "**For this part, you will need credentials from the Twitter dev website.**\n",
    "\n",
    "A good way to store your keys is using `.conf` files and `configparser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stuff/conf.conf']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(\"stuff/conf.conf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YOURS'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['twitter']['api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Section: twitter>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['twitter']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how my `conf.conf` file looks like (also in `stuff/conf_public.conf`):\n",
    "\n",
    "```\n",
    "[twitter]\n",
    "api_key = YOURS\n",
    "api_secret_key = YOURS\n",
    "access_token = YOURS\n",
    "access_secret_token = YOURS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A useful package: Tweepy\n",
    "\n",
    "https://tweepy.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtweepy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Tweepy Hello World\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# authentication (OAuth)\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[43mtweepy\u001b[49m\u001b[38;5;241m.\u001b[39mOAuthHandler(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapi_secret_key\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m auth\u001b[38;5;241m.\u001b[39mset_access_token(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccess_token\u001b[39m\u001b[38;5;124m'\u001b[39m], config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtwitter\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccess_secret_token\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m api \u001b[38;5;241m=\u001b[39m tweepy\u001b[38;5;241m.\u001b[39mAPI(auth)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "# Tweepy Hello World\n",
    "\n",
    "# authentication (OAuth)\n",
    "auth = tweepy.OAuthHandler(config['twitter']['api_key'], config['twitter']['api_secret_key'])\n",
    "auth.set_access_token(config['twitter']['access_token'], config['twitter']['access_secret_token'])\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets[:5]:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interlude: JSON\n",
    "\n",
    "The Twitter API returns data structured in the JSON format. [JSON](https://www.json.org) (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate. **It is basically a list of nested Python dictionaries.**\n",
    "\n",
    "\n",
    "Minimal example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"firstName\": \"John\",\n",
    "  \"lastName\": \"Doe\",\n",
    "  \"age\": 21\n",
    "}\n",
    "```\n",
    "\n",
    "Extended example:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"$id\": \"https://example.com/person.schema.json\",\n",
    "  \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "  \"title\": \"Person\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"firstName\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The person's first name.\"\n",
    "    },\n",
    "    \"lastName\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"The person's last name.\"\n",
    "    },\n",
    "    \"age\": {\n",
    "      \"description\": \"Age in years which must be equal to or greater than zero.\",\n",
    "      \"type\": \"integer\",\n",
    "      \"minimum\": 0\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "Online viewer: http://jsonviewer.stack.hu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: search\n",
    "\n",
    "All the most recent Tweets from a given hashtag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [46]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# queries\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mtweepy\u001b[49m\u001b[38;5;241m.\u001b[39mCursor(api\u001b[38;5;241m.\u001b[39msearch, q\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#nlproc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(item\u001b[38;5;241m.\u001b[39m_json)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "# queries\n",
    "\n",
    "tweets = tweepy.Cursor(api.search, q=\"#nlproc\")\n",
    "\n",
    "for item in tweets.items(2):\n",
    "    print(item._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [47]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# queries with Boolean operators\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tweets \u001b[38;5;241m=\u001b[39m \u001b[43mtweepy\u001b[49m\u001b[38;5;241m.\u001b[39mCursor(api\u001b[38;5;241m.\u001b[39msearch, q\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#nlproc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m tweets\u001b[38;5;241m.\u001b[39mitems(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(item\u001b[38;5;241m.\u001b[39m_json, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, sort_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "# queries with Boolean operators\n",
    "import json\n",
    "\n",
    "tweets = tweepy.Cursor(api.search, q=\"#nlproc\")\n",
    "\n",
    "for item in tweets.items(2):\n",
    "    print(json.dumps(item._json, indent=4, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: users\n",
    "\n",
    "Get some info on a given user, and explore their friends/followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241m.\u001b[39mget_user(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melonmusk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser:\u001b[39m\u001b[38;5;124m\"\u001b[39m, user\u001b[38;5;241m.\u001b[39mscreen_name)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "user = api.get_user(\"elonmusk\")\n",
    "\n",
    "print(\"User:\", user.screen_name)\n",
    "print(\"------\")\n",
    "print(\"Friends:\", user.friends_count)\n",
    "print(\"Followers:\", user.followers_count)\n",
    "print(\"------\")\n",
    "for friend in user.friends(count=10):\n",
    "    print(friend.screen_name)\n",
    "print(\"------\")\n",
    "for friend in user.followers(count=10):\n",
    "    print(friend.screen_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the API: tweets from user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [49]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m user \u001b[38;5;241m=\u001b[39m \u001b[43mapi\u001b[49m\u001b[38;5;241m.\u001b[39mget_user(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melonmusk\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m elon_tweets \u001b[38;5;241m=\u001b[39m user\u001b[38;5;241m.\u001b[39mtimeline()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m elon_tweets[:\u001b[38;5;241m10\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'api' is not defined"
     ]
    }
   ],
   "source": [
    "user = api.get_user(\"elonmusk\")\n",
    "elon_tweets = user.timeline()\n",
    "\n",
    "for tweet in elon_tweets[:10]:\n",
    "    print(tweet.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those who don't have a Twitter account and app, here are some tweets on and by Boris Johnson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mention_tweets(username, n=100):\n",
    "    \"\"\"\n",
    "    Get all tweet mentions by Twitter username.\n",
    "    \n",
    "    Args:\n",
    "        username (str): Twitter username\n",
    "        n (int, optional): number of tweets to get from timeline\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tweets (str) in which this user is mentioned\n",
    "    \"\"\"\n",
    "    \n",
    "    if username.startswith('@'):\n",
    "        user = username\n",
    "    else:\n",
    "        user = '@' + username\n",
    "    \n",
    "    mentions = []\n",
    "    tweets_on_user = tweepy.Cursor(api.search, q=user, tweet_mode=\"extended\")\n",
    "    \n",
    "    for tweet in tweets_on_user.items(n):\n",
    "        mentions.append(tweet.full_text)\n",
    "        \n",
    "    return mentions\n",
    "    \n",
    "    \n",
    "def get_user_tweets(username, n=100):\n",
    "    \"\"\"\n",
    "    Get a user's tweets by username.\n",
    "    \n",
    "    Args:\n",
    "        username (str): Twitter username\n",
    "        n (int, optional): number of tweets to get from timeline\n",
    "        \n",
    "    Returns:\n",
    "        list: List of tweets (str) from this user\n",
    "    \"\"\"\n",
    "    \n",
    "    user = api.get_user(username, tweet_mode=\"extended\") # extended tweetmode gets also the longer 280/char tweets\n",
    "    tweets_from_user = user.timeline(count=n)\n",
    "\n",
    "    tweets = []\n",
    "    for tweet in tweets_from_user:\n",
    "        tweets.append(tweet.text)\n",
    "        \n",
    "    return tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweepy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m on_boris \u001b[38;5;241m=\u001b[39m \u001b[43mget_mention_tweets\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBorisJohnson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m from_boris \u001b[38;5;241m=\u001b[39m get_user_tweets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBorisJohnson\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36mget_mention_tweets\u001b[0;34m(username, n)\u001b[0m\n\u001b[1;32m     16\u001b[0m     user \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m username\n\u001b[1;32m     18\u001b[0m mentions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 19\u001b[0m tweets_on_user \u001b[38;5;241m=\u001b[39m \u001b[43mtweepy\u001b[49m\u001b[38;5;241m.\u001b[39mCursor(api\u001b[38;5;241m.\u001b[39msearch, q\u001b[38;5;241m=\u001b[39muser, tweet_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextended\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tweet \u001b[38;5;129;01min\u001b[39;00m tweets_on_user\u001b[38;5;241m.\u001b[39mitems(n):\n\u001b[1;32m     22\u001b[0m     mentions\u001b[38;5;241m.\u001b[39mappend(tweet\u001b[38;5;241m.\u001b[39mfull_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tweepy' is not defined"
     ]
    }
   ],
   "source": [
    "on_boris = get_mention_tweets(\"BorisJohnson\")\n",
    "from_boris = get_user_tweets(\"BorisJohnson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'on_boris' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [52]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# note we are using the \"\" as text delimiter\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f_on_boris, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mon_boris\u001b[49m:\n\u001b[1;32m      8\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m t \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(f_from_boris, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'on_boris' is not defined"
     ]
    }
   ],
   "source": [
    "# Save to file (a one-column CSV)\n",
    "f_on_boris = \"stuff/tweets_on_boris.csv\"\n",
    "f_from_boris = \"stuff/tweets_from_boris.csv\"\n",
    "\n",
    "# note we are using the \"\" as text delimiter\n",
    "with open(f_on_boris, \"w\", encoding='utf-8') as f:\n",
    "    for t in on_boris:\n",
    "        f.write('\"' + t + '\"\\n')\n",
    "        \n",
    "with open(f_from_boris, \"w\", encoding='utf-8') as f:\n",
    "    for t in from_boris:\n",
    "        f.write('\"' + t + '\"\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.\n",
    "\n",
    "1. Download the last 100 (or another number) tweets mentioning a user you are interested into and the last 100 from the user itself. Alternatively, use the tweets in the on_boris and from_boris files.\n",
    "\n",
    "\n",
    "2. Create a minimal pipeline to normalize the tweets into lists of tokens.\n",
    "\n",
    "\n",
    "3. Count and compare from the two datasets, the most frequent (top 10):\n",
    "    - tokens\n",
    "    - hashtags\n",
    "    - other user mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}